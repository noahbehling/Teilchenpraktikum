{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection of $B_s \\rightarrow \\psi(2S)\\,K_S$ events: A lab exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will perform the (most interesting) part of a **real physics data analysis** of the $B_s \\rightarrow \\psi(2S)\\,K_S$ decay:  \n",
    "The filtering of the tiny fraction of interesting events from the vast amount of total data that is recorded in the LHCb experiment at CERN.  \n",
    "You will be learning about classification tasks in particle physics, utilizing machine learning algorithms and expert systems to squeeze out the best performance.  \n",
    "But let's not panic, we go through it step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work on three samples:\n",
    "- The real data set.\n",
    "- Monte-Carlo simulation of the signal decay $B_s \\rightarrow \\psi(2S)\\,K_S$.\n",
    "- Monte-Carlo simulation of the (kinematically) similar decay $B_d \\rightarrow \\psi(2S)\\,K_S$.\n",
    "\n",
    "\n",
    "Let's first define the paths to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python plotting library\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual data\n",
    "path_to_data = \"/data/Bs2psi2SKS/data.root\"\n",
    "#signal simulation\n",
    "path_to_sim = \"/data/Bs2psi2SKS/signal_sim.root\"\n",
    "#control channel simulation\n",
    "path_to_control_sim = \"/data/Bd2psi2SKS/control_sim.root\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nomenclature here is:\n",
    " - data = the actual data coming from LHCb\n",
    " - signal sim = monte-carlo simulation of our signal decay $B_s \\rightarrow \\psi(2S)\\,K_S$\n",
    " - control sim = monte-carlo simulation of the similar decay $B_d \\rightarrow \\psi(2S)\\,K_S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data from a \"ROOT\" file\n",
    "\n",
    "Everything at CERN works with [ROOT](https://root.cern.ch), also the output of our distributed data processing chain.  \n",
    "ROOT is incredibly powerful, but also not too easy to use..  \n",
    "Let's not try to make it work, but rather use [uproot](https://github.com/scikit-hep/uproot) to put our data into [pandas dataframes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, chunk_size=None, treename = 'Tree', branches = None):\n",
    "    import numpy as np\n",
    "    import uproot\n",
    "    if not chunk_size:\n",
    "        return uproot.open(path)[treename].pandas.df(branches=branches, flatten=False).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    for data in uproot.open(path)[treename].pandas.iterate(entrysteps = chunk_size, flatten=False, branches=branches):\n",
    "        return data.replace([np.inf, -np.inf], np.nan).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "Let's have a look at our simulation first.\n",
    "\n",
    "**Ex 1.1: Get a dataframe of the signal simulation and plot the invariant $B_s$ mass.**  \n",
    "<h4>Hint: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">The name is B_FitDaughtersConst_M_flat</a></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your plot should show a peaking structure near the nominal $B_s$ mass: $m(B_s) = 5366.88\\,\\text{MeV}$.\n",
    "\n",
    "\n",
    "Let's do the same for our data:\n",
    "\n",
    "**Ex 1.2: Plot the invariant $B_s$ mass of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the LHC is running, millions of collisions happen every second at LHCb, resulting in 40Tb/s of data. The data you see here contains events from the entire Run 2 of the LHC (2015 - 2018), but obviously already **heavily preselected** by the trigger and other centralized selections.\n",
    "\n",
    "Still, the amount of (for our purposes) uninteresting events is still overwhelming, and this is what you see in the mass distribution. The $B_s \\rightarrow \\psi(2S)\\,K_S$ signal events are hiding somewhere in this mess. And besides the peaking structure (which does not correspond to the $B_s$), the data sample is dominated by **combinatorial background**.\n",
    "\n",
    "Our goal is to find the needles in the haystack here, so lets get on with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an **outline** of the tasks ahead:\n",
    " 2. Define a **signal window** and a **background sample**\n",
    " 3. **Select features** that discriminate signal and background well\n",
    " 4. Build an algorithm that tries to **predict how signal-like** an event is, using these features\n",
    " 5. **Optimize a threshold** to split the data into signal and background\n",
    " 6. Apply the classification to the signal window in our data and **fit** the resulting distribution to\n",
    "    quantify how clearly we can make out $B_s \\rightarrow \\psi(2S)\\,K_S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Define signal and background training samples\n",
    "\n",
    "We will train our classifier on signal and background samples.\n",
    "\n",
    "### 2.1 Signal window and background sample\n",
    "**Ex 2.1:**  \n",
    "**Define a window in which we expect signal to appear: The shortest interval that contains 99\\% of the signal-MC mass distribution and visualize your result**\n",
    "\n",
    "<h4>Hint: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\"> Compute all intervalls containing 99% and take the shortest.</a></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined a signal window, lets see where it is in the data mass distribution.  \n",
    "\n",
    "**Ex 2.2: Plot the data mass distribution with the signal window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the classifier, we should never use the signal window because it could bias the classifier. Instead, we need to define suitable signal and background training samples to work on.  \n",
    "Our signal sample will be the signal simulation, and as background we want to choose the **upper sideband (USB)** of the $B_s$ peak (yet invisble) in the data. The USB of the $B_s$ peak contains the events with reconstructed $B_s$ masses larger than the nominal $B_s$ mass. It contains only combinatorial background and is therefore perfect for the job, since this is what we want to get rid of.  \n",
    "\n",
    "**Ex 2.3: Select the background sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Signal training sample\n",
    "Our simulation algorithms are far from perfect, mainly due to imperfect theoretical physics models and our imperfect modelling of the detector to simulate responses. For example, some aspects of the decay kinematics are often imperfect in simulation and are corrected for using such weights, stored in the variable `kinematic_weights`. You should use these weights for everything from now on.\n",
    "\n",
    "But other properties may be mis-modelled by the simulation as well. This means that even if you find the most discriminating variables now (using the`kinematic_weights`) and train a classifier with them, your classifier would only be able to classify simulation and data - instead of signal and background.\n",
    "\n",
    "Therefore, we need to evaluate how similar simulation and data are in each variable without having $B_s$ data to compare the simulation with. And here we take advantage of the decay $B^0\\to\\psi(2S)K_S$. The $B^0$ decay is similar to the $B_s$ decay, especially in the kinematic variables which will be very important in the removal of combinatorial background. Additionally, the $B^0$ decay is very abundant, we can already see it by eye in the data set.\n",
    "\n",
    "Fortunately, the sWeights `sWeights_sig` giving us pure $B^0$ events from the data sample have already been computed.\n",
    "**Caution:** sWeighted data distributions are only a reliable representation of the true distribution if the variable you look at is uncorrelated to the invariant $B$ mass.\n",
    "\n",
    "**Ex. 2.4: Plot the reconstructed invariant mass of the $B$ meson in the data set with and without sWeights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 2.5: Compare the $B^0$ events in data with the $B^0$ simulation for each variable by eye**\n",
    "\n",
    "<h4>Hint: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\"> Don't forget the weights in both samples.</a></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature selection\n",
    "Now we want to find a set of variables that are most suitable to identify the $B_s$ events in the data sample. To this end, the variables need to have two properties:\n",
    " 1. Reasonable agreement between data and simulation (otherwise the classifier only learns to distinguish data from simulation instead of signal from background).\n",
    " 2. Strong discrimination between signal (simulation) and background (USB).\n",
    " 3. Uncorrelated to the invariant mass of the $B_s/B^0$ candidate (otherwise the decisions regarding 1. made on the control channel using sWeights may be wrong AND the fit to the $B_s$ invariant mass in the end may be biased).\n",
    "\n",
    "The eventually chosen most suitable variables are called features.\n",
    "\n",
    "**This section provides a step-by-step guide to feature selection which is perfectly fine to use. But of course you are encouraged to combine all or parts of the exercises below. There is no single best solution.**\n",
    "\n",
    "First, we identify the variables that are well modelled in the simulation. This is similar to Ex. 2.5 but instead of comparing distributions by-eye, you should choose a more objective -- read \"quantitative\" -- measure.\n",
    "\n",
    "**Ex 3.1: Determine a metric to quantify the similarity between $B^0$ simulation and $B^0$ data. Use this metric to identify variables that are not suitable for the classification because they are mis-modelled in simulation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we are left with all variables that could help to discriminate between signal and background. The second step of feature selection is identifying the discriminatory power of each remaining variable.\n",
    "\n",
    "**Ex. 3.2: Define a metric to quantify discriminating power and choose the best features to use in a classification algorithm.**\n",
    "\n",
    "<h4>Hint: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">This is very similar to Ex. 3.1.</a></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should have a set of variables that is well simulated and provides good discrimination between signal and background. Before we move on, you should check whether these features are uncorrelated to the invariant mass of the $B_s$ candidate.\n",
    "\n",
    "**Ex. 3.3: Compute the correlation between your features and the invariant $B_s$ mass and remove features from your set if they are too correlated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a classification algorithm!\n",
    "\n",
    "Now we have everything we need to train our classification algorithm. Awesome!\n",
    "\n",
    "**Again, this section is a rather close guide to training a BDT. As before, feel free to ignore the exercises and define a classification algorithm deviating from the help provided. If you are feeling especially curious, try out other binary classification algorithms too.**\n",
    "\n",
    "I suggest to use the [XGBoost](https://github.com/dmlc/xgboost) implementation of a [Boosted](https://en.wikipedia.org/wiki/Gradient_boosting) [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree) for starters.\n",
    "\n",
    "A BDT is a [supervised learning algorithm](https://en.wikipedia.org/wiki/Supervised_learning). This means that it needs to know to which category its input truely belongs. Therefore, we need to define labels for each event. The proposed implementation of a BDT takes an array of training samples and a list containing the corresponding label. \n",
    "\n",
    "**Ex 4.1: Define training sample and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is way more entries in our background sample than in the signal simulation. If these numbers are too unbalanced, classification algorithms may run into problems. Consider for instance a training set with 99% background. By simply classifiying the entire sample as background, the algorithm achieves 99% accuracy. That's not very useful.\n",
    "Therefore, we assign a weight to each background event to make them \"less important\" in the training, while still taking advantage of the full backround sample.\n",
    "\n",
    "**Ex 4.2: Define suitable weights for the background sample**\n",
    "\n",
    "<h4>Hint: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">50/50 is never a bad choice.</a></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you choose to use the implementation proposed above, you can define and train a BDT like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "bdt = XGBClassifier()\n",
    "bdt.fit(training_samples,\n",
    "        training_labels,\n",
    "        sample_weight = training_weights,\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array of (signal and background) training samples is called `training_samples`, the list of labels is called `training_labels`, and the weights for each event is contained in the list `training_events`.\n",
    "\n",
    "In order to avoid overtraining, we want split the data to train the classifier on one part and classify\n",
    "only the \"unseen\" part. Since we do not want to \"waste\" any data just for testing,\n",
    "we [cross-validate](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29) using the implementation in `sklearn.model_selection.StratifiedKFold`.\n",
    "\n",
    "**Make sure that you always apply the right BDT to the right training sample (the one you trained it on)!**\n",
    "\n",
    "**Ex 4.3: Define a folding and train one BDT per fold, training on $\\frac{n-1}{n}$ and testing on $\\frac{1}{n}$ of the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual a plot can give you a quick idea of what you are doing is correct.\n",
    "\n",
    "**Ex 4.4: Plot the distribution of the BDT output for the singal and background samples to visualize the classification.**\n",
    "<h4>\n",
    "Hint: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\"> Make sure to apply the right BDT to the right sample. And a logarithmic scale may be useful again.\n",
    "</a></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model consisting of $n$ BDTs is trained, we want to check how well it did! As metric we choose to look at the [Reciever-Operating-Characteristic (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) and the area under its curve.\n",
    "\n",
    "**Ex 4.5: Evaluate the performance of your classifier by plotting the ROC curve and calculating the area under the curve for every BDT you trained. Do not evaluate the signal region of the data yet.**\n",
    "<h4>\n",
    "Hint: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">\n",
    "Make sure you apply the right BDt to the right sample. Check out `sklearn` for an implementation of the ROC curve.\n",
    "</a></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification threshold optimization\n",
    "Now that we have our predictions, we would like to define a value at which we split the datasets into signal and background. We go with the following definition of a measure for signal sensitivity/significance:   \n",
    "$s = \\frac{\\epsilon_{sig}}{5/2 + \\sqrt{n_{bkg}}}$,  \n",
    "where $\\epsilon_{sig}$ is the efficiency of our selection regarding signal and $n_{bkg}$ is the number of background events in the signal region.\n",
    "\n",
    "We compute the significance for several possible splitting points from 0 to 1. Eventually, we choose the cut-off leading to the highest significance.\n",
    "\n",
    "First though, we need to compute the signal efficiency and number of background events in the signal region serving as input to our FOM.\n",
    "\n",
    "**Ex 5.1: Find a way to estimate the signal efficiency and the number of background events in the signal region(!) depending on the cut on the BDT output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 5.2: Determine the best cut value based on your just defined metric.**\n",
    "<h4>\n",
    "Hint: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">\n",
    "You could for example average the best cut values of the $n$ BDTs to combine them to a total value.\n",
    "</a></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Estimation of the number of $B_s$ events in the data sample\n",
    "Now that we defined a best cut, lets apply our classification algorithm to the full dataset and see how well it works. We will then get the \"real\" signal significance by fitting the selected mass distribution.  \n",
    "\n",
    "\n",
    "**Ex 6.1: Apply your classifier to the full dataset and plot the mass distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see two peaks when plotting on a log-scale.\n",
    "\n",
    "Now we have to find out what efficiency vs background retention we are getting here. Therefore, we need to model the entire distribution. This model should consist of two peaking structures ($B^0$ and $B_s$) and one exponential background component. To build a stable model, we first fit the peaks to their respective simulation and fix the obtained shape parameters in the fit to data. To ensure that the peaks in simulation match the peak shape in data, you need to classify both signal samples and apply the BDT cut before performing the fit.\n",
    "\n",
    "I recommend using the [`zfit`](https://zfit.readthedocs.io/en/latest/) library for fitting. But again, this is just one of many good choices that you are free to take.\n",
    "\n",
    "**Ex 6.2: Fit a peak to the signal simulation.**\n",
    "<h4>Hint: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\"> Maybe ONE Gaussian is not enough?</a></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the shape parameters of the signal mass peak, we do the same for the $B^0$ peak.\n",
    "\n",
    "**Ex 6.3: Fit a peak to the BDT selected $B^0$ simulation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have set up an awesome classifier but it may not be able to catch all combinatorial background in the data sample. Therefore, you should also include a background component into your model. A common choice for combinatorial background is a decreasing exponential function.\n",
    "\n",
    "**Ex 6.4: Fit the full data mass with the fixed peak shapes and an exponential background.**\n",
    "<h4>\n",
    "Hint: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">\n",
    "Don't forget to include a scale factor for each component.\n",
    "</a></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 6.5: Plot the fully fitted model, its 3 submodels and the selected data distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! We see our $B_s$ peak!\n",
    "\n",
    "Let's evaluate the statistical sigificance of our observation. The correct way of doing this is rather complicated and tedious which is beyond the scope of what we can and want to do here. Instead, we will use\n",
    "$m = \\frac{n_{sig}}{\\sqrt{n_{sig}+n_{bkg}}}$\n",
    "as a simple proxy for the statistical significance. The numbers $n_i$ are the number of signal/background events in the signal region.\n",
    "\n",
    "**Ex 6.6: Calculate $n_{sig}$, $n_{bkg}$, and the significance proxy $m$!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations! You did it. Now you are more than ready for a thesis at E5 :)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
